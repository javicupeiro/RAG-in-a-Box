import ollama
import logging
from typing import Optional, List

logger = logging.getLogger(__name__)

class OllamaClient:
    """
    A client to interact with the Ollama API.
    Manages communication, message formatting, and errors.
    """
    def __init__(self, host: str, model: str):
        """
        Initializes the Ollama client.

        Args:
            host (str): The URL of the host where Ollama is running (e.g., 'http://localhost:11434').
            model (str): The name of the model to use (e.g., 'llava' or 'granite-vision').
        """
        try:
            self.client = ollama.Client(host=host)
            self.model = model
            logger.info(f"Ollama client initialized for model '{self.model}' on host '{host}'.")
            # Check that the model exists
            self.client.show(model) 
            logger.info(f"Model '{self.model}' verified and available in Ollama.")
        except Exception as e:
            logger.error(f"Error initializing Ollama client or verifying model '{model}': {e}")
            logger.error("Make sure Ollama is running and you have done 'ollama pull <model_name>'.")
            raise

    def generate_response(self, prompt: str, images_base64: Optional[List[str]] = None) -> str:
        """
        Generates a response from a prompt, optionally with images.

        Args:
            prompt (str): The text prompt for the LLM.
            images_base64 (Optional[List[str]]): A list of images encoded in base64.

        Returns:
            str: The response generated by the model.
        """
        try:
            logger.debug(f"Generating response for model '{self.model}'. Prompt: '{prompt[:100]}...'")
            
            # Ollama expects images to be passed in the first message of the conversation.
            messages = [
                {
                    'role': 'user',
                    'content': prompt,
                    'images': images_base64 if images_base64 else []
                }
            ]

            response = self.client.chat(
                model=self.model,
                messages=messages
            )
            
            content = response['message']['content']
            logger.debug(f"Response received: '{content[:100]}...'")
            return content.strip()

        except Exception as e:
            logger.error(f"Error during response generation with Ollama: {e}")
            return "Sorry, an error occurred while communicating with the language model."

